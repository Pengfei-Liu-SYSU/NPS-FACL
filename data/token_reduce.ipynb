{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f18595b-8238-46eb-b7aa-503bd85e8a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer_1 = AutoTokenizer.from_pretrained('../ckpts/molt5-base')\n",
    "tokes1id = tokenizer_1('CC1(C(C1(C)C)C(=O)C2=CN(C3=CC=CC=C32)CC4CCCCN4C)C')\n",
    "print(tokes1id)\n",
    "tokes1 = tokenizer_1.tokenize('CC1(C(C1(C)C)C(=O)C2=CN(C3=CC=CC=C32)CC4CCCCN4C)C')\n",
    "print(tokes1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e6fc0c0-336e-46a1-bd5f-13f8d5cb9840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [3, 2823, 536, 599, 254, 599, 254, 536, 599, 254, 61, 254, 61, 32100, 205, 357, 2423, 10077, 41, 254, 519, 2423, 254, 32105, 3274, 254, 2668, 61, 2823, 591, 254, 32108, 314, 254, 61, 254, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['▁', 'CC', '1', '(', 'C', '(', 'C', '1', '(', 'C', ')', 'C', ')', 'C(=O)', '▁C', '2', '=', 'CN', '▁(', 'C', '3', '=', 'C', 'C=CC', '▁=', 'C', '32', ')', 'CC', '4', 'C', 'CCCN', '▁4', 'C', ')', 'C']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer_2 = AutoTokenizer.from_pretrained('../ckpts/molt5-base-add')\n",
    "tokes2id = tokenizer_2('CC1(C(C1(C)C)C(=O)C2=CN(C3=CC=CC=C32)CC4CCCCN4C)C')\n",
    "print(tokes2id)\n",
    "tokes2 = tokenizer_2.tokenize('CC1(C(C1(C)C)C(=O)C2=CN(C3=CC=CC=C32)CC4CCCCN4C)C')\n",
    "print(tokes2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc5aa278-2ac6-429d-9762-c41a7375115d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = 'benchmark.csv'  \n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "filtered_df = df[df['NPS'] == 1]\n",
    "\n",
    "smiles_list = filtered_df['smiles'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cd7c08f-c365-4888-a36b-e8cf9cc6db7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Count Before Optimization: 41052\n",
      "Token Count After Optimization: 32705\n",
      "Token Count Reduction: 20.33%\n"
     ]
    }
   ],
   "source": [
    "def calculate_token_count(smiles_list, tokenizer):\n",
    "    total_tokens = 0\n",
    "\n",
    "    for smiles in smiles_list:\n",
    "        tokens = tokenizer.tokenize(smiles)  \n",
    "        total_tokens += len(tokens)\n",
    "    \n",
    "    return total_tokens\n",
    "\n",
    "\n",
    "token_count_before = calculate_token_count(smiles_list, tokenizer_1)\n",
    "token_count_after = calculate_token_count(smiles_list, tokenizer_2)\n",
    "\n",
    "\n",
    "token_reduction = (token_count_before - token_count_after) / token_count_before if token_count_before > 0 else 0\n",
    "\n",
    "print(f\"Token Count Before Optimization: {token_count_before}\")\n",
    "print(f\"Token Count After Optimization: {token_count_after}\")\n",
    "print(f\"Token Count Reduction: {token_reduction * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6688c8af-cd0a-4b32-af51-87a32b606bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Molecules: 1228\n",
      "Molecules with Reduced Tokens: 1189\n",
      "Percentage of Molecules with Reduced Tokens: 96.82%\n"
     ]
    }
   ],
   "source": [
    "def calculate_token_count(smiles_list, tokenizer_1, tokenizer_2):\n",
    "    total_tokens = 0\n",
    "    reduced_token_count = 0  \n",
    "\n",
    "    for smiles in smiles_list:\n",
    "        tokens_before = tokenizer_1.tokenize(smiles)  \n",
    "        tokens_after = tokenizer_2.tokenize(smiles)   \n",
    "        \n",
    "        total_tokens += 1  \n",
    "        \n",
    "        if len(tokens_after) < len(tokens_before):\n",
    "            reduced_token_count += 1  \n",
    "\n",
    "    return total_tokens, reduced_token_count\n",
    "\n",
    "\n",
    "total_molecules, reduced_molecules = calculate_token_count(smiles_list, tokenizer_1, tokenizer_2)\n",
    "\n",
    "\n",
    "reduction_percentage = (reduced_molecules / total_molecules) * 100 if total_molecules > 0 else 0\n",
    "\n",
    "print(f\"Total Molecules: {total_molecules}\")\n",
    "print(f\"Molecules with Reduced Tokens: {reduced_molecules}\")\n",
    "print(f\"Percentage of Molecules with Reduced Tokens: {reduction_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8da97c-4b76-413d-b253-86df807fee16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
